---
title: "Range Mapper R Walkthrough"
author: "Adrian George, Sydney Widell, Robert Roth, and Jack Williams"
affiliation: "University of Wisconsin-Madison"
date: "12/28/2021"

---

##Introduction

This walkthrough will guide you through downloading records from the [Neotoma Paleoecology Database](https://www.neotomadb.org), so you can make your own online maps of plants, fossils, testate amoeba, and more. To extend this approach to other taxa, regions, and times, fork the [Range Mapper Github repository](https://github.com/NeotomaDB/RangeMapper) and modify the code, as directed below. If you wish to recreate Range Mapper, go to 

Want to see what the final product could look like? Visit [Range Mapper](http://open.neotomadb.org/RangeMapper/index.html), the original set of visualizations mapping plant taxon range shifts from the Last Glacial Maximum to present in North America, Europe, and Australasia. The current interactive maps are only a few of many that could be possible with code that links Neotoma's deep paleoecological data repository to CARTO's CartoVL JavaScript library.

Our code integrates previous methods established by @williams2017neotoma and @goring2018bulk-baconizing for extracting and interpolating Neotoma data in R. In this guide, we will provide step-by-step instructions for running all parts of the code.

#### RStudio

Many people working with R may choose to use [RStudio](http://rstudio.com).  If you are using RStudio, you can customize elements of this document (for example, removing this header information) and then use the *knit* button to compile the code into an HTML document, while generating the necessary output files (see the [README file](https://github.com/neotomadb) in the GitHub repository).

## Setting up your library

The `neotoma2` package facilitates interactions between R and the Neotoma Paleoecology Database, and the `dplyr` package allows you to filter the data you've extracted by factors like site and age.

```{r library}
# First, install the neotoma2 R package. Comment out the next two lines if you have it.
library(devtools)
devtools::install_github('NeotomaDB/neotoma2')

# Load packages
library(dplyr)
library(neotoma2)
```

## Retrieving Neotoma data

With the set-up complete, you're ready to download your first dataset! In this example, we'll compile pollen records from Oceania with southern beech (*Nothofagus*) and eucalyptus (*Eucalyptus*), but there are countless taxa available from Neotoma. You can browse for taxa on [Neotoma Explorer](https://apps.neotomadb.org/explorer/) or use the function, `neotoma::get_taxa()`. Be aware that these taxa names are case-sensitive.

Then, select the geographical bounds of your query and express them as coordinates. The [Bounding Box website](https://boundingbox.klokantech.com/) is useful for generating your own bounding regions. Otherwise, these coordinates worked well for us for North America, Europe, and the Indo-Pacific:

 North American bounding box: `location <- c(-130, 24, -34, 65)`
 European bounding box: `location <- c(-11, 35, 47, 72)`
 Indo-Pacific bounding box: `location <- c(105, -51, 177, 10)`

It's also possible to select data from a geopolitical region. Instructions are in the `neotoma2` package documentation. Use the method that works best for you.

Third, set your boundary ages. We set the minimum to 21ka BP and maximum to 0.

These parameters are recorded in the metadata for each record.


```{r }
# set the location on your computer where you want to save your files
file_path <- '' # [USER INPUT]

# Set your taxa, location bounding box, and time period of interest
# Look at the Range_Mapper_methods.Rmd file for more
# Oceania
location <- list(geoJSON = '{"type": "Polygon",
        "coordinates": [[
            [105, -51],
            [105, 10],
            [177, 10],
            [177, -51],
            [105, -51]]]}',
        bbox = c(105, -51, 177, 10))

# Set taxa names to search in Neotoma
taxa <- c('Nothofagus','Eucalyptus') # [USER INPUT]

# Set common names (or names you want on the map legend) for your taxa here
common <- c('Southern Beech', 'Eucalyptus') # [USER INPUT]

# Set the time period [USER INPUT]
age_young <- 0
age_old <- 21000


```


## Appending and downloading your lists

This section downloads information for each record that matches the parameters, and then the record numbers are used to download all your records as a list. Set your own file path in the last line of code.

```{r selecting and downloading datasets}

# Runs code only if the pollen data file isn't in your file directory
if (!'PollenRecords.RData' %in% list.files(paste0(file_path))) {
  # Get dataset information for pollen core and pollen surface samples for all taxa
  run = TRUE
  offset <- 0

  while(run) {
    datasets <- get_datasets(datasettype = "pollen", loc = location$geoJSON, offset=offset, limit = 500)
    if(length(datasets) == 0) {
      run = FALSE
    }
    if(exists('all_datasets')) {
      all_datasets <- c(all_datasets, datasets)
    } else {
      all_datasets <- datasets
    }
    offset <- offset + 500
  }

  # filters datasets without age models
  filter_datasets <- all_datasets %>% 
    neotoma2::filter(!is.na(age_range_young))
  
  # Creates empty "sites" object for downloads
  all_downloads <- new('sites')

  # downloads all datasets
  for (i in 1:length(filter_datasets)) {
      all_downloads <- c(all_downloads, get_downloads(filter_datasets[i]))
  }
} else {
  all_downloads <- readRDS(paste0(file_path,"PollenRecords.RData")) # Imports existing data file
}

pollen_sites <- get_sites(loc = location$geoJSON, all_data = TRUE)
neotoma2::plotLeaflet(filter_datasets) %>% 
  leaflet::addPolygons(map = ., 
                       data = location$sf, 
                       color = "green")
View(neotoma2::summary(pollen_sites))

pollen_datasets <- neotoma2::get_datasets(pollen_sites, datasettype = "pollen", all_data = TRUE)
all_downloads <- get_downloads(pollen_sites, all_data = TRUE) 

```


## Compliling Data

In this next section, you'll be working with the records you've downloaded from Neotoma. You'll retrieve that data and prepare it for mapping. You will remove aquatic taxa and non-pollen taxa. Then, you'll select records that have calibrated radiocarbon chronologies.

```{r compile}

# Selects samples w/calibrated radiocarbon chronologies & non-aquatic pollen counts between -250 and 21250 BP
samp_filtered <- all_samp %>% 
  dplyr::filter(ecologicalgroup %in% c("UPHE", "TRSH") & elementtype == "pollen" & units == "NISP") %>%
  dplyr::filter(agetype != "Radiocarbon years BP" & agetype != "NA") %>%
  dplyr::filter(age <= 21250 & age >= -250)
  
# Harmonizes taxa names
samp_harmonized <- samp_filtered
for (i in 1:length(taxa)) {
  samp_harmonized <- samp_harmonized %>%
  mutate(variablename = replace(variablename, 
                                stringr::str_detect(variablename, paste0(taxa[i], "*")), 
                                taxa[i]))
}



```

## Linear Interpolation & Final CSV

The following code fetches total counts for each taxon observation at every site and linerally interpolates all the data in 500 year intervals.

```{r include = TRUE}

# If you want to make a new version, add "_V2" to the file name, so R will rerun the following code chunk.
if (!'CartoInput_Aus.csv' %in% list.files(paste0(file_path))) { 
  tot.cnts <- rowSums(compiled_pollen.clean[,11:ncol(compiled_pollen.clean)], na.rm=TRUE)
  
  # Interpolates the pollen to 500 year intervals, creating a line for each observation of each taxon
  interp_dl <- data.frame(compiled_pollen.clean[,1:10],
                          time = - (round(compiled_pollen.clean$age / 500, 0) * 500),
                          nothofagus = compiled_pollen.clean[, grep("Nothofagus*", colnames(compiled_pollen.clean))] / tot.cnts,
                          eucalyptus = compiled_pollen.clean[, grep("Eucalyptus*", colnames(compiled_pollen.clean))] / tot.cnts) %>%
    group_by(time, lat, long, site.name) %>%
    summarize(Nothofagus = mean (nothofagus) * 100,
              Eucalyptus = mean (eucalyptus) * 100)
  

  
  # Reduces dataset to info needed for mapping  
  plant.data <- data.frame()
  for (i in 1:length(common)) {
    taxon          <- common[i]
    samples        <- interp_dl[,i+4]
    names(samples) <- "samples"
    taxa           <- rep(taxon,nrow(samples))
    current.taxon.data <- cbind(interp_dl[,1:4],samples,taxa)
    plant.data <- rbind(plant.data,current.taxon.data)
  }
  colnames(plant.data)[colnames(plant.data) == '...6'] <- 'taxa'
  plant.data <- plant.data[order(plant.data$time,plant.data$site.name),]
  
  # Removes any observations from over 21,000 years ago
  timefltr_output <- dplyr::filter(plant.data, time >= -21000)
  final_output <- na.omit(timefltr_output)

  # Writes CSV file
  write.csv(final_output, file = paste0(file_path,'CartoInput_Aus.csv')) # To make a new version, add "_V2" to file name, so you don't overwrite the original.
}
```
Next, you will need to log into your CARTO account and upload the data. If you are a student or educator, you can create a free CARTO account by following the [instructions here](https://docs.carto.com/faqs/categories/carto-for-education/). Now you're ready to create some rad visualizations!
